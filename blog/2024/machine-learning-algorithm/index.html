<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Machine Learning Algorithm | Tuo Liu </title> <meta name="author" content="Tuo Liu"> <meta name="description" content="ML algorithm"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tuoooliu666.github.io/blog/2024/machine-learning-algorithm/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tuo</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Machine Learning Algorithm</h1> <p class="post-meta"> Created on July 12, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/machinelearning"> <i class="fa-solid fa-hashtag fa-sm"></i> MachineLearning</a>   ·   <a href="/blog/category/machinelearning"> <i class="fa-solid fa-tag fa-sm"></i> MachineLearning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <ul> <li>statistical perspective: \(Y=f(X)+\epsilon\) <ul> <li>model=algorithm(data)</li> </ul> </li> <li>computer science perspective: <ul> <li>output = program(input)</li> <li>prediction = program(instance)</li> </ul> </li> <li>algorithms that learn a mapping from input to output <ul> <li>ML algorithms are techniques for estimating target function f to predict the output (Y) given input variables (X)</li> <li>different ML algorithms make different assumptions about the shape and structure of the target function and how best to optimize a representation to approximate it, which makes fitting and comparing a suite of algorithms necessary.</li> </ul> </li> </ul> <h2 id="algorithms--models">Algorithms &amp; Models</h2> <ul> <li>linear models <ul> <li>linear regression</li> <li>generalized linear models (GLMs)</li> <li>logistic regression (LR)</li> <li>Naive Bayes classifers (NB)</li> <li>linear discriminant analysis (LDA)</li> </ul> </li> <li>tree-based methods <ul> <li>decision trees</li> <li>random forest</li> <li>gradient boosting</li> </ul> </li> <li>instance-based methods <ul> <li>kNN</li> <li>SVM</li> </ul> </li> <li>neural networks <ul> <li>feedforward neural networks (NN)</li> <li>convolutional neural networks (CNN)</li> <li>recurrent neural networks (RNN)</li> </ul> </li> </ul> <h3 id="binary-classification">Binary classification</h3> <ul> <li>input vector \(x \in \mathbb{R}^d\)</li> <li>output \(y \in \{0,1\}\)</li> <li>the goal is to construct a function \begin{equation}f: \mathcal{X} \rightarrow {0,1} \end{equation}</li> </ul> <p>using 0-1 loss, the risk of a classifier \(f: \mathcal{X} \rightarrow Y\) is given by:</p> <p>\begin{equation} R(f)=EPE(f)E_{X,Y} \mathcal{1}(Y \ne f(X)) = P(Y \ne f(X)) \end{equation}</p> <p>the Bayes rule \(f^{*}\) relies on the posterior probabilities</p> <p>\begin{equation} f^{*}=\arg \min R(f)=\begin{cases} 1 \quad if \quad P(Y=1|X=x) &gt; P(Y=0|X=x) <br> 0 \quad if \quad P(Y=1|X=x) &lt;&gt; P(Y=0|X=x) \end{cases} \end{equation}</p> <p>the Bayes risk is defined as the risk of \(f^{*}\), which has the smallest possible risk among all possible classifiers</p> <p>\begin{equation} R(f^{<em>})=P(Y \ne f^{</em>}(X))=P(Y=1)P(Y=0|X=x)+P(Y=0)P(Y=1|X=x) \end{equation}</p> <h4 id="example-rare-disease">example: rare disease</h4> <p>define class 1 = “disease”, 0 = “disease-free”. \(\pi_1=1\%, \pi_0=99\%\).</p> <table> <tbody> <tr> <td>recall the Bayes rule $$f(x)=\mathcal{1}(P(Y=1</td> <td>X=x) &gt; P(Y=0</td> <td>X=x))$$</td> </tr> </tbody> </table> <ul> <li> <table> <tbody> <tr> <td>posterioe class probability P(Y=j</td> <td>X=x) gives updated probabilities after observing x</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>if $$P(Y=1</td> <td>X=x) &gt; 0.5$$, thenwe randomly assign data to one class.</td> </tr> </tbody> </table> </li> </ul> <p>Example: Assume a certain rare disease occurs among 1% of the population. There is a test for this disease: 99.5% of the disease will test positive, and only 0.5% of the disease-free group will test positive. (We assume the false positive and false negative rate are both 0.005.) Now a person comes with a positive test result. What is the prediction rule?</p> <p>the conditional probability of X given Y is</p> \[P(X=+|Y=1)=0.995, P(X=-|Y=0)=0.005 \\ P(X=+|Y=0)=0.005, P(X=-|Y=0)=0.995\] <p>using Bayes’ Theorem,</p> \[P(Y=1|X=+) = \frac{P(X=+|Y=1)P(Y=1)}{P(X=+)} = \frac{P(X=+|Y=1)P(Y=1)}{P(X=+|Y=1)P(Y=1)+P(X=+|Y=0)P(Y=0)} = \frac{0.995*0.01}{0.995*0.01+0.005*0.99} = 0.668 \\ P(Y=0|X=+) = \frac{P(X=+|Y=0)P(Y=0)}{P(X=+)} = \frac{0.005*0.99}{0.995*0.01+0.005*0.99} = 0.332\] <table> <tbody> <tr> <td>Since P(Y = 0</td> <td>X = +) = 0.332 &lt; 0.668, the Bayes rule assigns a person with the “+” test result to class “disease”.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Similarly, P(Y = 0</td> <td>X = −) = 0.9999, P(Y = 1</td> <td>X = −) = 0.0001, so the Bayes rule assigns a person with the “-” test result to class “disease-free”.</td> </tr> </tbody> </table> <h4 id="unequal-cost">Unequal cost</h4> <p>the Bayes rule under unequal cost is given by</p> <p>\begin{equation} f^{*}(x)= \begin{cases} 1 \quad if \quad C(1,0)P(Y=1|X=x) &gt; C(0,1)P(Y=0|X=x) <br> 0 \quad if \quad C(1,0)P(Y=1|X=x) &lt; C(0,1)P(Y=0|X=x) \end{cases} \end{equation}</p> <h3 id="linear-classification-methods">Linear classification methods</h3> <p>two popular linear classifiers</p> <ul> <li>Linear Discriminant Analysis (LDA)</li> <li>Logistic Regression models (LR) <ul> <li>both models rely on the linear-odd assumption, indirectly or directly.</li> <li>LDA and LR estimate the coefficients in a different ways.</li> </ul> </li> </ul> <p>linear-logit model (LDA and LR): assume that the logit is linear in x:</p> <p>\begin{equation} \log \frac{P(Y=1|X=x)}{P(Y=0|X=x)}=w^Tx+b \end{equation}</p> <p>posterior probability:</p> <p>\begin{equation} P(Y=1|X=x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}=\frac{1}{1+e^{-w^Tx-b}} <br> P(Y=0|X=x)=\frac{1}{1+e^{w^Tx+b}} \end{equation}</p> <table> <tbody> <tr> <td>under equal-cost, the decision boundary is given by $${x</td> <td>w^Tx+b=0}={x</td> <td>P(Y=1</td> <td>X=x)=0}$$.</td> </tr> </tbody> </table> <h4 id="lda">LDA</h4> <p>LDA assumptions</p> <ul> <li> <table> <tbody> <tr> <td>each class density is multivariate Guassian: $$X</td> <td>Y_j \sim N(\mu_j, \sigma_j)$$</td> </tr> </tbody> </table> </li> <li>Equal covariance matrices for each class: \(\sigma_j = \sigma\)</li> </ul> <p>under mixture Guassian assumption, the log-odds is expressed as:</p> <p>\begin{equation} \log \frac{P(Y=1|X=x)}{P(Y=0|X=x)}=\log \frac{\pi_1 \phi(x|\mu_1, \Sigma)/m(x)}{\pi_0 \phi(x|\mu_0, \Sigma)/m(x)} <br> = \log \frac{\pi_1}{\pi_0} + \log \phi(x|\mu_1, \Sigma)- \log \phi(x|\mu_0, \Sigma) <br> = \log \frac{\pi_1}{\pi_0} - \frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1) + \frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0) <br> = \log \frac{\pi_1}{\pi_0} - \frac{1}{2}(\mu_1+\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0) + x^T\Sigma^{-1}(\mu_1-\mu_0) <br> \log \frac{\pi_1}{\pi_0} - \frac{1}{2}(\mu_1+\mu_0)^T\beta_1+x^T\beta_1 \end{equation}</p> <p>under 0-1 loss, the Bayes rule is: assign 1 to x if and only if:</p> \[\log \frac{P(Y=1|X=x)}{Y=0|X=x}&gt;0\] <p>which is equivalent to: assign 1 to x if and only if:</p> \[\bigg [ \log \frac{\pi_1}{\pi_0} - \frac{1}{2}(\mu_1+\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0) \bigg ] + x^T\Sigma^{-1}(\mu_1-\mu_0) &gt; 0.\] <h4 id="lr">LR</h4> <ul> <li> <table> <tbody> <tr> <td>denote $$\mu=E(Y</td> <td>X)=P(Y=1</td> <td>X)$$.</td> </tr> </tbody> </table> </li> <li>assuming \(g(\mu)=\log \frac{\mu}{1-\mu}=\beta_0+\beta_1^TX\).</li> </ul> <h4 id="high-dimensional-classifiers">high-dimensional classifiers</h4> <ul> <li>LDA-type <ul> <li>Naive Bayes, Nearest Shrunken Centroid (NSC)</li> <li>sparse LDA, regularized LDA</li> </ul> </li> <li>penalized logistic regression</li> <li>large-margin methods <ul> <li>support vector machines (SVM)</li> </ul> </li> <li>classification tree <ul> <li>decision tree</li> <li>random forest</li> </ul> </li> <li>boosting</li> </ul> <h3 id="nonlinear-classifier">Nonlinear classifier</h3> <ul> <li>KNN</li> <li>kernal SVM</li> <li>trees, random forests</li> <li>…</li> </ul> <h4 id="k-nearest-neighbor-knn-classifiers">K-Nearest Neighbor (KNN) classifiers</h4> <p>the degree of freedom is n/k</p> <ul> <li>k controls the model complexity <ul> <li>smaller k: lower bias and higher variance</li> <li>larger k: higher bias and lower varance (reduce effect of noise)</li> </ul> </li> <li> <p>the best choice of k depends upon the data</p> </li> <li>pros <ul> <li>geenrally low bias</li> <li>no stringent assumptions about data</li> <li>robust against outliers</li> <li>works well for large n small d</li> </ul> </li> <li>cons <ul> <li>potentially high variance</li> <li>work bad for large d small n</li> <li>accuracy severely degraded by noisy or irrelevant features, or the feature scales are not consistent with their importance <ul> <li>Much research effort on selecting or scaling features to improve classification, such as optimize feature scaling.</li> </ul> </li> </ul> </li> </ul> <h4 id="kernal-svm">kernal SVM</h4> <h4 id="tree-based-method">tree-based method</h4> <h3 id="ensemble-methods">Ensemble methods</h3> <ul> <li>bagging</li> <li>boosting</li> <li>stacking</li> </ul> <h3 id="neural-network">neural network</h3> <h4 id="forward-feeding-neural-network">forward feeding neural network</h4> <h4 id="convolutional-neural-network-cnn">convolutional neural network (CNN)</h4> <h4 id="recurrent-neural-network-rnn">recurrent neural network (RNN)</h4> <p>RNN is a special case of feed-forward NN with a single hidden layer, where the hidden layer is a recurrent layer.RNN is a special case of CNN with a single filter.</p> <ul> <li>What is the purpose of gated recurrent unit (GRU) in a recurrent neural network (RNN)?</li> </ul> <p>The purpose of a gated recurrent unit (GRU) in a recurrent neural network (RNN) is to address the vanishing gradient problem and capture long-range dependencies in sequential data.</p> <p>The vanishing gradient problem occurs when the gradients used to update the weights in an RNN diminish exponentially as they propagate back through time, making it difficult for the network to learn long-term dependencies. The GRU helps mitigate this problem by introducing gating mechanisms that control the flow of information within the network.</p> <p>The GRU has two main gates: the update gate (Gamma_u) and the reset gate (Gamma_r). The update gate determines how much of the previous memory should be retained and how much of the new input should be incorporated. The reset gate decides how much of the previous memory should be forgotten.</p> <p>By adaptively updating and resetting the memory cell, the GRU can selectively retain important information and discard irrelevant information, allowing it to capture long-range dependencies more effectively. This makes the GRU a powerful tool for tasks such as natural language processing, speech recognition, and time series analysis.</p> <ul> <li>How does a gated recurrent unit (GRU) work?</li> </ul> <p>Sure! Let’s break down the topic of gated recurrent units (GRUs) in a recurrent neural network (RNN) in simple terms.</p> <p>Imagine you’re reading a sentence and trying to understand its meaning. As you read each word, you need to remember important information from earlier in the sentence to make sense of it. The same goes for a computer trying to process sequential data, like sentences or time series.</p> <p>The problem is that traditional RNNs struggle to remember long-range connections and can’t capture important information from earlier in the sequence. This is where GRUs come in.</p> <p>GRUs are like memory cells in the RNN. They help the network remember important information from earlier in the sequence and use it to make predictions or understand the data better. They do this by using two gates: the update gate and the reset gate.</p> <p>The update gate decides how much of the previous memory to keep and how much new information to incorporate. It helps the network decide what’s important to remember and what can be forgotten.</p> <p>The reset gate determines how much of the previous memory to forget. It helps the network reset or update the memory cell based on the current input.</p> <p>By using these gates, GRUs can selectively retain important information and discard irrelevant information, allowing the network to capture long-range connections and dependencies in the data. This makes GRUs very useful for tasks like understanding language, recognizing speech, and analyzing time series data.</p> <ul> <li> <p>What is the role of attention mechanism in sequence models?</p> </li> <li> <p>Explain the concept of word embeddings in natural language processing.</p> </li> </ul> <p>Word embeddings are a way of representing words as dense vectors in a continuous vector space. They are used in natural language processing (NLP) to capture the semantic and syntactic relationships between words.</p> <p>Word embeddings are trained on large text corpora and learn to represent words based on their context. For example, the word “king” might be represented as a vector that is close to the vector for “queen” and far from the vector for “car.”</p> <ul> <li>What is the transformer network and how does it improve upon traditional sequence models?</li> </ul> <p>The transformer network is a type of deep learning model that is used for processing sequential data, such as text or time series. It is based on the concept of self-attention, which allows the model to focus on different parts of the input sequence when making predictions.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/photo-gallery/">a post with image galleries</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/statistics-notes/">Statistics Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/ML-notes/">Machine Learning Notes</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Tuo Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>