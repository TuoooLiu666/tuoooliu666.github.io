<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Machine Learning Notes | Tuo Liu </title> <meta name="author" content="Tuo Liu"> <meta name="description" content="ML Notes"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tuoooliu666.github.io/blog/2024/ML-notes/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tuo</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Machine Learning Notes</h1> <p class="post-meta"> Created on July 12, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/machinelearning"> <i class="fa-solid fa-hashtag fa-sm"></i> MachineLearning</a>   ·   <a href="/blog/category/machinelearning"> <i class="fa-solid fa-tag fa-sm"></i> MachineLearning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="statistical-decision-theory">Statistical Decision Theory</h1> <h2 id="statistical-inference">statistical inference</h2> <p>data \(Z=(Z_1, \dots, Z_n)\) follow the distribution \(f(z|\theta)\)</p> <ul> <li>\(\theta \in \Theta\) is the parameter of interest, but unknown. It represents uncertainties.</li> <li>\(\theta\) is a scalar, vector, or matrix</li> <li>\(\Theta\) is the set containing all possible values of \(\theta\)</li> </ul> <p>the goal is to estimate \(\theta\) using the data, i.e., to construct \(\hat{\theta}(Z)\).</p> <ul> <li>it combines the sampling information (data) with a knowledge of the consequences of our decisions.</li> <li>major types of inference: <ul> <li>point estimation</li> <li>interval estimation</li> <li>hypothesis testing</li> </ul> </li> </ul> <h2 id="loss--risk">loss &amp; risk</h2> <ul> <li>loss function: \(\mathcal{L(\theta, \hat{\theta}): \Theta \times \Theta}\Rightarrow R.\) <ul> <li> \[L(\theta, \hat{\theta})\ge 0\] </li> <li>quantifies the consequence for each decision \(\hat{\theta}\)</li> <li>\(L(\theta, \hat{\theta}(Z))\) is a function of Z, and it is a random quantity</li> <li>examples <ul> <li>squared error loss: \(L(\theta, \hat{\theta})=(\theta-\hat{\theta})^2\)</li> <li> <table> <tbody> <tr> <td>absolute error loss: $$ L(\theta, \hat{\theta})=</td> <td>\theta-\hat{\theta}</td> <td>$$</td> </tr> </tbody> </table> </li> <li>0-1 loss: \(L(\theta, \hat{\theta})=\begin{cases}0, \quad \hat{\theta}=\theta \\ 1, \quad \hat{\theta}\neq \theta\end{cases}\)</li> </ul> </li> </ul> </li> <li>risk function: expected loss <ul> <li>\(R(\theta, \hat{\theta}(Z))=E_Z[L(\theta, \hat{\theta})]=\int_{Z}L(\theta, \hat{\theta}(Z))f(z\theta)dz\).</li> <li>\(R(\theta, \hat{\theta})\) is a deterministic function of \(\theta\)</li> <li> \[R(\theta, \hat{\theta}) \ge 0\] </li> <li>used to evaluate the overall performance of one estimator; compare two estimators; find the best estimator</li> <li>example from regression <ul> <li> \[L(\theta, \hat{\theta})=(\theta-\hat{\theta}(Z))^2\] </li> <li>\(R(\theta, \hat{\theta})=E_{Z}[\theta-\hat{\theta}]^2\), also known as the <strong>mean squared error</strong> (MSE)</li> </ul> </li> <li>example from binary classification <ul> <li>0-1 loss \(L(\theta, \hat{\theta})=\mathcal{1}(\theta \ne \hat{\theta}(Z))\)</li> <li>\(R(\theta, \hat{\theta})=E\mathcal{1}[\theta \ne \hat{\theta}(Z)]=\mathcal{P}(\theta \ne \hat{\theta}(Z))\), also known as the <strong>misclassification error rate</strong> </li> <li> </li> </ul> </li> </ul> </li> </ul> <h2 id="mse--bias-variance-tradeoff">MSE &amp; bias-variance tradeoff</h2> <ul> <li> \[bias(\hat{\theta}) = E(\hat{\theta})-\theta\] </li> <li> \[var(\hat{\theta})=E[\hat{\theta}-E(\hat{\theta})]^2\] </li> <li>decomposition of MSE <ul> <li> \[MSE = Bias^2[\hat{\theta}(Z)] + var[\hat{\theta}(Z)]\] </li> <li>both bias and variance contribute to the risk</li> </ul> </li> </ul> <h2 id="risk-comparison--best-estimator">Risk comparison &amp; best estimator</h2> <ul> <li>best estimator <ul> <li>\(\hat{\theta}^*(Z)\) is the estimator that minimizes the risk for all possible values of \(\theta\)</li> <li> \[\hat{\theta}^*(Z) = \arg\min_{\hat{\theta}} R(\theta, \hat{\theta}(Z))\] </li> </ul> </li> </ul> <p>in practice, such estimator is not always available and there exists no uniformly best estimator. To avoid this difficulty, we restrict the estimators to be in a class C, i.e.,</p> <ul> <li>C={unibased estimators}, i.e., \(E[\hat{\theta}(Z)]=\theta\)</li> <li>C={linear decision rules}</li> </ul> <p>UMVUE from unbiased estimators; best linear unbiased estimator (BLUE) from linear decision rules.</p> <p>in addition, the risk function is a function of the unknown parameter, not easy to use. alternative methods to compare risk include <strong>maximum risk</strong> and <strong>Bayes risk</strong>.</p> <h2 id="bayes-risk--minimax-risk">Bayes risk &amp; minimax risk</h2> <ul> <li>maximum risk &amp; minimax rule <ul> <li>a decision rule that minimizes the maximum risk</li> <li>MinMax rule focuses on the worst-case risk; thus very conservative</li> </ul> </li> <li>Bayes risk <ul> <li>Bayes risk is the risk of the Bayes estimator</li> <li>Bayes rule: Bayes estimators are the minimizers of the Bayes risk</li> </ul> </li> </ul> <h2 id="learning-theory-for-supervised-learning">Learning Theory for Supervised Learning</h2> <h2 id="supervised-learning">Supervised learning</h2> <ul> <li>input vector \(X \in R^p\)</li> <li>output Y is either discrete- ot continuous-valued</li> <li>the pair (X, Y) follow a joint distribution \((X, Y) \sim P(X, Y)\)</li> </ul> <p>training data set:</p> <ul> <li> \[\{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\} \sim i.i.d P(X, Y)\] </li> <li>each data point carries some information about the population</li> </ul> <p>the goal is to estimate the underlying relationship between X and Y,</p> \[f: X \Rightarrow Y\] <p>for future prediction</p> <ul> <li>For regression, \(f: \mathcal{R^p \Rightarrow R}\)</li> <li>For K-class classification, \(f: \mathcal{R^p \Rightarrow \{1,\cdots, K\}}\)</li> </ul> <p>we would like f(X) to be as close as possible to Y, i.e., \(f(X) \approx Y\).</p> <h2 id="optimal-learner">optimal learner</h2> <p>similar to decision theory, we minimize a learning loss function L to find</p> <ul> <li>measure discrepancy between Y and f(X)</li> <li>minimize the error of predicting Y</li> </ul> <p>risk R(f) is the expected loss, or Expected Prediction Error (EPE). we seek the best \(f\) by minimizing the risk R(f) over the entire population:</p> \[f^{*}=\arg \min R(f)\] <p>optimal learner:</p> <ul> <li>the optinal solution \(f^{*}\) depends on the loss function L</li> <li>the optimal solution \(f^{*}\) depends on the joint distribution \((X, Y) \sim P(X, Y)\); if the joint distribution is unknown, the optimal solution is unachievable.</li> <li>Bayes risk is the smallest possible risk, which is the risk of the Bayes estimator/optimal learner.</li> </ul> <p>Bayes rule for binary classification</p> <ul> <li>soft and hard classification rules <ul> <li> <table> <tbody> <tr> <td>soft classifier: obtain $$\hat{P}(Y=1</td> <td>X=x)\(, compare\)\hat{P}(Y=1</td> <td>X=x)$$ with .5</td> </tr> </tbody> </table> <ul> <li>examples: logistic regression, trees, LDA</li> </ul> </li> <li> <table> <tbody> <tr> <td>hard classifier: directly estimate $$1(P(Y=1</td> <td>X=x)&gt;.5)$$</td> </tr> </tbody> </table> <ul> <li>examples: SVM</li> </ul> </li> </ul> </li> </ul> <h2 id="empirical-risk-minimization">empirical risk minimization</h2> <p>since P(X,Y) is unknown, the risk function R(f) is also unknown. we estimate the risk by the <strong>empirical risk</strong>:</p> \[R_{emp}(f) = \frac{1}{n} \sum_{i=1}^n L(f(x_i), y_i)\] <ul> <li>ERM for regression <ul> <li>loss function \(L(Y,f(X))=[Y-f(X)]^2\)</li> <li>empirical risk \(R_{emp}(f)=RSS(f)=\sum_{i=1}^{n}[y_i-f(x_i)]^2\), also known as residual sum of squares (RSS)</li> </ul> </li> </ul> <h2 id="restricted-estimators">restricted estimators</h2> <ul> <li>overfitting: RSS=0 <ul> <li>solution: restricted estimators to a class &amp; penalization</li> </ul> </li> <li>pre-specified class <ul> <li>parametric model class: the form of f is known, only parameters are unknown</li> <li>nonparametric model class: the form of f is unspecified</li> </ul> </li> <li>penalization: <ul> <li>penalized RSS = RSS + $$\lambda J(f)$</li> </ul> </li> </ul> <h2 id="epe-interpretation">EPE interpretation</h2> <p>EPE = estimation error + approximation error</p> <ul> <li>estimation error: the error of estimating the parameters of f, due to the finite training data <ul> <li>contribute to variance</li> </ul> </li> <li>approximation error: the error of approximating f, possible that the empirical function is not in the specified class <ul> <li>contribute to bias</li> </ul> </li> </ul> <h1 id="data-issues">Data issues</h1> <ul> <li>missing values <ul> <li>SVM, regularized models, neural networks cannot tolerate any missing values</li> <li>CART and Naive Bayes can handle missing values</li> </ul> </li> <li>be on vastly different scales</li> <li>follow a skewed distribution</li> <li>contain a small number of extreme values</li> <li>be censored on the low and/or high end of the range</li> <li>have a complex relationship with the response and be truly predictive but cannot be adequately represented with a simple function or extracted by sophisticated models</li> <li>contain relevant and overly redundant information</li> </ul> <h1 id="feature-engineering">Feature Engineering</h1> <p>some machine learning algrithms require feature engineering, such as tree-based algorithms or the data to be in a specific form. Some algorithms perform better if the data is prepared in a specific way. Additionally, the raw data may not be in the best position to expose the underlying structure and relationships to the responses.</p> <ul> <li>Feature Engineering <ul> <li>Feature Construction <ul> <li>categorical encoding <ul> <li>dummy variables: Creating dummy predictors may not be the most effective way of extracting predictive information from a categorical predictor.</li> <li>factors</li> </ul> </li> <li>numerous variables <ul> <li>tree-based methods are immune to skewness and outliers</li> <li>linear models, kNN, SVM are sensitive to skewness and outliers</li> <li>MLR, nueral networks are adversely impacted by high correlations while partial least squares are designed for it</li> </ul> </li> </ul> </li> <li>Feature Extraction</li> <li>Feature Selection</li> <li>Feature Transformation <ul> <li> <strong>regression methods</strong> work better when the features are standarized or normalized</li> </ul> </li> <li>Feature Scaling <ul> <li> <strong>instance based methods</strong> are more effective if the features have the same scale</li> </ul> </li> <li>Feature Interaction</li> </ul> </li> <li>linear models <ul> <li>linear regression</li> <li>logistic regression (LR)</li> <li>linear discriminant analysis (LDA)</li> </ul> </li> <li>tree-based methods <ul> <li>decision trees</li> <li>random forest</li> <li>gradient boosting</li> </ul> </li> <li>instance-based methods <ul> <li>kNN</li> <li>SVM</li> </ul> </li> <li>neural networks <ul> <li>feedforward neural networks</li> <li>convolutional neural networks</li> <li>recurrent neural networks</li> </ul> </li> </ul> <h2 id="binary-classification">Binary classification</h2> <ul> <li>input vector \(x \in \mathbb{R}^d\)</li> <li>output \(y \in \{0,1\}\)</li> <li>the goal is to construct a function \begin{equation}f: \mathcal{X} \rightarrow {0,1} \end{equation}</li> </ul> <p>using 0-1 loss, the risk of a classifier \(f: \mathcal{X} \rightarrow Y\) is given by:</p> <p>\begin{equation} R(f)=EPE(f)E_{X,Y} \mathcal{1}(Y \ne f(X)) = P(Y \ne f(X)) \end{equation}</p> <p>the Bayes rule \(f^{*}\) relies on the posterior probabilities</p> <p>\begin{equation} f^{*}=\arg \min R(f)=\begin{cases} 1 \quad if \quad P(Y=1|X=x) &gt; P(Y=0|X=x) <br> 0 \quad if \quad P(Y=1|X=x) &lt;&gt; P(Y=0|X=x) \end{cases} \end{equation}</p> <p>the Bayes risk is defined as the risk of \(f^{*}\), which has the smallest possible risk among all possible classifiers</p> <p>\begin{equation} R(f^{<em>})=P(Y \ne f^{</em>}(X))=P(Y=1)P(Y=0|X=x)+P(Y=0)P(Y=1|X=x) \end{equation}</p> <h3 id="example-rare-disease">example: rare disease</h3> <p>define class 1 = “disease”, 0 = “disease-free”. \(\pi_1=1\%, \pi_0=99\%\).</p> <table> <tbody> <tr> <td>recall the Bayes rule $$f(x)=\mathcal{1}(P(Y=1</td> <td>X=x) &gt; P(Y=0</td> <td>X=x))$$</td> </tr> </tbody> </table> <ul> <li> <table> <tbody> <tr> <td>posterioe class probability P(Y=j</td> <td>X=x) gives updated probabilities after observing x</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>if $$P(Y=1</td> <td>X=x) &gt; 0.5$$, thenwe randomly assign data to one class.</td> </tr> </tbody> </table> </li> </ul> <p>Example: Assume a certain rare disease occurs among 1% of the population. There is a test for this disease: 99.5% of the disease will test positive, and only 0.5% of the disease-free group will test positive. (We assume the false positive and false negative rate are both 0.005.) Now a person comes with a positive test result. What is the prediction rule?</p> <p>the conditional probability of X given Y is</p> \[P(X=+|Y=1)=0.995, P(X=-|Y=0)=0.005 \\ P(X=+|Y=0)=0.005, P(X=-|Y=0)=0.995\] <p>using Bayes’ Theorem,</p> \[P(Y=1|X=+) = \frac{P(X=+|Y=1)P(Y=1)}{P(X=+)} = \frac{P(X=+|Y=1)P(Y=1)}{P(X=+|Y=1)P(Y=1)+P(X=+|Y=0)P(Y=0)} = \frac{0.995*0.01}{0.995*0.01+0.005*0.99} = 0.668 \\ P(Y=0|X=+) = \frac{P(X=+|Y=0)P(Y=0)}{P(X=+)} = \frac{0.005*0.99}{0.995*0.01+0.005*0.99} = 0.332\] <table> <tbody> <tr> <td>Since P(Y = 0</td> <td>X = +) = 0.332 &lt; 0.668, the Bayes rule assigns a person with the “+” test result to class “disease”.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Similarly, P(Y = 0</td> <td>X = −) = 0.9999, P(Y = 1</td> <td>X = −) = 0.0001, so the Bayes rule assigns a person with the “-” test result to class “disease-free”.</td> </tr> </tbody> </table> <h3 id="unequal-cost">unequal cost</h3> <p>the Bayes rule under unequal cost is given by</p> <p>\begin{equation} f^{*}(x)= \begin{cases} 1 \quad if \quad C(1,0)P(Y=1|X=x) &gt; C(0,1)P(Y=0|X=x) <br> 0 \quad if \quad C(1,0)P(Y=1|X=x) &lt; C(0,1)P(Y=0|X=x) \end{cases} \end{equation}</p> <h2 id="linear-classification-methods">Linear classification methods</h2> <p>two popular linear classifiers</p> <ul> <li>Linear Discriminant Analysis (LDA)</li> <li>Logistic Regression models (LR) <ul> <li>both models rely on the linear-odd assumption, indirectly or directly.</li> <li>LDA and LR estimate the coefficients in a different ways.</li> </ul> </li> </ul> <p>linear-logit model (LDA and LR): assume that the logit is linear in x:</p> <p>\begin{equation} \log \frac{P(Y=1|X=x)}{P(Y=0|X=x)}=w^Tx+b \end{equation}</p> <p>posterior probability:</p> <p>\begin{equation} P(Y=1|X=x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}=\frac{1}{1+e^{-w^Tx-b}} <br> P(Y=0|X=x)=\frac{1}{1+e^{w^Tx+b}} \end{equation}</p> <table> <tbody> <tr> <td>under equal-cost, the decision boundary is given by $${x</td> <td>w^Tx+b=0}={x</td> <td>P(Y=1</td> <td>X=x)=0}$$.</td> </tr> </tbody> </table> <h3 id="lda">LDA</h3> <p>LDA assumptions</p> <ul> <li> <table> <tbody> <tr> <td>each class density is multivariate Guassian: $$X</td> <td>Y_j \sim N(\mu_j, \sigma_j)$$</td> </tr> </tbody> </table> </li> <li>Equal covariance matrices for each class: \(\sigma_j = \sigma\)</li> </ul> <p>under mixture Guassian assumption, the log-odds is expressed as:</p> <p>\begin{equation} \log \frac{P(Y=1|X=x)}{P(Y=0|X=x)}=\log \frac{\pi_1 \phi(x|\mu_1, \Sigma)/m(x)}{\pi_0 \phi(x|\mu_0, \Sigma)/m(x)} <br> = \log \frac{\pi_1}{\pi_0} + \log \phi(x|\mu_1, \Sigma)- \log \phi(x|\mu_0, \Sigma) <br> = \log \frac{\pi_1}{\pi_0} - \frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1) + \frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0) <br> = \log \frac{\pi_1}{\pi_0} - \frac{1}{2}(\mu_1+\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0) + x^T\Sigma^{-1}(\mu_1-\mu_0) <br> \log \frac{\pi_1}{\pi_0} - \frac{1}{2}(\mu_1+\mu_0)^T\beta_1+x^T\beta_1 \end{equation}</p> <p>under 0-1 loss, the Bayes rule is: assign 1 to x if and only if:</p> \[\log \frac{P(Y=1|X=x)}{Y=0|X=x}&gt;0\] <p>which is equivalent to: assign 1 to x if and only if:</p> \[\bigg [ \log \frac{\pi_1}{\pi_0} - \frac{1}{2}(\mu_1+\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0) \bigg ] + x^T\Sigma^{-1}(\mu_1-\mu_0) &gt; 0.\] <h3 id="lr">LR</h3> <ul> <li> <table> <tbody> <tr> <td>denote $$\mu=E(Y</td> <td>X)=P(Y=1</td> <td>X)$$.</td> </tr> </tbody> </table> </li> <li>assuming \(g(\mu)=\log \frac{\mu}{1-\mu}=\beta_0+\beta_1^TX\).</li> </ul> <h3 id="high-dimensional-classifiers">high-dimensional classifiers</h3> <ul> <li>LDA-type <ul> <li>Naive Bayes, Nearest Shrunken Centroid (NSC)</li> <li>sparse LDA, regularized LDA</li> </ul> </li> <li>penalized logistic regression</li> <li>large-margin methods <ul> <li>support vector machines (SVM)</li> </ul> </li> <li>classification tree <ul> <li>decision tree</li> <li>random forest</li> </ul> </li> <li>boosting</li> </ul> <h2 id="nonlinear-classifier">Nonlinear classifier</h2> <ul> <li>KNN</li> <li>kernal SVM</li> <li>trees, random forests</li> <li>…</li> </ul> <h3 id="nearest-neighbor-nn-classifiers">Nearest Neighbor (NN) classifiers</h3> <h3 id="kernal-svm">kernal SVM</h3> <h1 id="model-selection">Model Selection</h1> <ul> <li>Cross Validation <ul> <li>K-fold Cross Validation</li> <li>Stratified K-fold Cross Validation</li> <li>Leave-One-Out Cross Validation</li> <li>Repeated K-fold Cross Validation</li> <li>Nested Cross Validation</li> </ul> </li> </ul> <h1 id="assessing-model-performance">Assessing-Model-Performance</h1> <h2 id="regression">Regression</h2> <ul> <li>Mean Absolute Error (MAE)</li> <li>Mean Squared Error (MSE)</li> <li>Root Mean Squared Error (RMSE)</li> <li>R-squared (R^2)</li> <li>Adjusted R-squared (R^2_adj)</li> </ul> <h2 id="classification">Classification</h2> <ul> <li>Confusion Matrix</li> <li>Accuracy</li> <li>ROC Curve: True Positive Rate (TPR) vs False Positive Rate (FPR)</li> <li>AUC: Area Under the ROC Curve</li> <li>Precision</li> <li>Recall</li> <li>Precision-recall Curve</li> <li>F1 Score</li> </ul> <h3 id="confusion-matrix">confusion matrix</h3> <table> <thead> <tr> <th>Actual \ Predicted</th> <th>Positive</th> <th>Negative</th> </tr> </thead> <tbody> <tr> <td>Positive</td> <td>TP</td> <td>FN</td> </tr> <tr> <td>Negative</td> <td>FP</td> <td>TN</td> </tr> </tbody> </table> <ul> <li> \[\mathcal{accuracy = \frac{(TP + TN)}{(TP + TN + FP + FN)}}\] </li> <li>balanced data <ul> <li>sensitivity = TP / (TP + FN)</li> <li>specificity = TN / (TN + FP)</li> <li>true positive rate (TPR) = sensitivity = TP / (TP + FN)</li> <li>false positive rate (FPR) = 1- specificity = FP / (FP + TN)</li> <li> <strong>ROC curve</strong> <ul> <li>True Positive Rate (TPR) vs. False Positive Rate (FPR)</li> <li>sensitivity vs. (1-specificity)</li> </ul> </li> <li>AUC: Area Under the ROC Curve</li> </ul> </li> <li>imbalanced data <ul> <li> \[\mathcal{precision = sensitivity = \frac{TP}{(TP + FP)}}\] </li> <li> \[\mathcal{recall = \frac{TP}{(TP + FN)}}\] </li> <li>F1 Score = 2 * (precision * recall) / (precision + recall)</li> <li> <strong>precision-recall curve</strong>: True Positive Rate (TPR) vs False Positive Rate (FPR)</li> </ul> </li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/photo-gallery/">a post with image galleries</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/statistics-notes/">Statistics Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/machine-learning-algorithm/">Machine Learning Algorithm</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Tuo Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>