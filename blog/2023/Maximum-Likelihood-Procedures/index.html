<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Maximum Likelihood Procedures | Tuo Liu </title> <meta name="author" content="Tuo Liu"> <meta name="description" content="multivariate-analysis"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tuoooliu666.github.io/blog/2023/Maximum-Likelihood-Procedures/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tuo</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Maximum Likelihood Procedures</h1> <p class="post-meta"> Created on February 16, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/linearalgebra"> <i class="fa-solid fa-hashtag fa-sm"></i> LinearAlgebra</a>   <a href="/blog/tag/datascience"> <i class="fa-solid fa-hashtag fa-sm"></i> DataScience</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="maximum-likelihood-and-multivariate-normal-distribution">Maximum Likelihood and Multivariate Normal Distribution</h2> <h3 id="model-parameter-objective-function-and-optimization">Model, Parameter, Objective Function, and Optimization</h3> <p>Any procedure is underlain by a model that can be expressed as</p> \[\tag{1} Data \cong \phi(\Theta)+Errors\] <p>where \(\Theta\) denotes the parameters to be estimated. For instance, in KMC cluster analysis $\Theta$ is [\(\mathbb{G, C}\)]. In regression analysis, the \(\Theta=[\vec{b},c]\) and \(\phi(\vec{b},c)=\mathbb{X}\vec{b}+c\vec{1}\).</p> <p>An analysis procedure modeled as (1) estimates parameter \(\Theta\) values. This is formulated as-Obtaining \(\Theta\) that optimizes an objective function obj(\(\Theta\)) subject to a constraint on \(\Theta\).</p> <p>Here, the term “optimizes” refers to either “minimizes” or “maximizes”, and some function can be used as obj(\(\Theta\)). In <em>least squares method</em>, the least squares are used as obj(\(\Theta\)), which are generally expressed as \(\lVert\mathbb{X}-\phi(\Theta)\rVert^2\), with “optimizes” referring to “minimizes” and \(\Theta=[\vec{b},c]\) is not constrained.</p> <h3 id="maximum-likelihood-method">Maximum Likelihood Method</h3> <p>A maximum likelihood (ML) method can be formulated by rephrasing“optimizing” and “an objective function” as “maximizing” and “probability”, respectively. To note, ML uses notion of probabilities, which is not used in the LS method.</p> <p>An simple example illustrating ML: suppose a black box contains black and white balls, where the total number of balls is known to be 100, but the number of black or white balls is unknown. We use \(\theta\) for the number of black ones. In order to estimate \(\theta\), a ball was drawn from the box and returned back to the box five times, which produces the following data set</p> \[\vec{d}=[1,0,0,1,0]^T\] <p>Here, \(d_i=1, d_i=0\) indicate black and white balls drawn, respectively, with \(d_i\) denotes the ith element of \(\vec{d}\).</p> <p>The probability of \(d_i = 1\) observed (i.e., a black ball chosen) and that of \(d_i = 0\) are expressed as</p> \[P(d_i=1|\theta)=\frac{\theta}{100} \\ P(d_i=0|\theta)=1-\frac{\theta}{100}\] <p>Further, we suppose the balls were chosen mutually independently. Then, the probability of the data set \(\vec{d}\) is</p> \[P(\vec{d}|\theta)=(\frac{\theta}{100})^2(1-\frac{\theta}{100})^3\] <p>For estimation of the value of \(\theta\), the ML method can be used. The idea of the method can be stated as “Obtaining the parameter value such that the occurrence of an event is the most likely”. Here, the “event” refers to the observation of a data set, i.e., observing \(\vec{d}\) and “how likely it is that the event will occur” is measured by its <em>probability</em>.</p> <p>Note that \(P(\vec{d}\mid\theta)\) is treated as a function of parameter \(\theta\) for a fixed \(\vec{d}\) in the ML method. The probability, if it is treated as a function of parameters, is rephrased as likelihood, from which the name maximum likelihood method originates. A solution in the ML method is called a maximum likelihood estimate (MLE).</p> <h3 id="probability-density-function">Probability Density Function</h3> <p><strong>Clarification</strong>: in general, probability mass function (PMF) is used in the context of discrete random variables, while the probability density function (PDF) is used in the context of continuous random variables.</p> <p>The PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value.This probability is given by the integral of this variable’s PDF over that range (area). The probability density function is nonnegative everywhere, and the area under the entire curve is equal to 1.</p> <p>In the example above, we have a variable that can take on discrete values as 1 and 0. The probability of a genuinely continuous variable being a specific value cannot reasonably be defined. However, the probability can be defined for the intervals of a continuous variable by denoting the probability of x taking values within the interval of \((a, b)\) as \(P(x \pm \delta)\). The density of the probability is given as</p> \[Pr(a \le x \le b)=\int_{a}^{b}f_X(x)dx\] <p>where \(f_X(x)\) denotes the probability density function of X. If \(F_X\) is the cumulative distribution function of X, then</p> \[f_X(x)=\frac{d}{dx}F_X(x)\] <p>Intuitively, one can think of \(f_{X}(x)dx\) as being the probability of X falling within the infinitesimal interval \([x,x+dx]\).</p> <h3 id="multivariate-normal-distribution">Multivariate Normal Distribution</h3> <p>For multivariate analysis, a PDF for multiple variables is needed. For example, in order to express how likely a person’s height, weight, and waist circumference are \([170.6, 65.3, 80.7]\), we need to define the distribution of \(\mathbf{x}=\begin{bmatrix} \vec{h} &amp; \vec{w} &amp; \vec{wc} \end{bmatrix}\).</p> <p>A commonly used PDF for multiple variables is \(\begin{equation} \tag{2} \mathcal{P} (\mathbf{x}|\vec{\mu}, \mathbb{\Sigma}) = \frac{1}{(2\pi)^{(p/2)} |\mathbb{\Sigma}|^{1/2}} \exp \left\{- \frac{1}{2} \left(\mathbf{x}-\vec{\mu}\right)^{T} \mathbb{\Sigma}^{-1} \left(\mathbf{x}-\vec{\mu} \right) \right\} \end{equation}\)</p> <p>where</p> <ul> <li>design matrix represented by \(\mathbf{x}=[\vec{x_1}, \cdots, \vec{x_p}]_{(n \times p)}\)</li> <li>mean vector \(\vec{\mu}_{(p \times 1)}\)</li> <li>covariance matrix \(\mathbb{\Sigma}_(p \times p)\), and \(\mid\Sigma\mid\) denotes the determinant of \(\Sigma\).</li> </ul> <p>We denote a multivariate normal (MVN) distribution of p variables as \(\vec{x} \sim N_p(\vec{\mu}, \mathbb{\Sigma})\)</p> <p>where \(\vec{\mu}\) denotes the vector of means and \(\mathbb{\Sigma}\) denotes the covariance matrix. If p=2, i.e., \(\mathbb{X}=[x_1, x_2]\), the probability density function (bivariate normal distribution) of X is resembling a bell in 3-dimensional space.</p> <h3 id="maximum-likelihood-method-for-normal-variables">Maximum Likelihood Method for Normal Variables</h3> <p>In (2), the PDF for MVN distribution is based on the assumption that \((\vec{\mu}, \mathbb{\Sigma})\) are known. But in practise, they are often unknown and what we have is observed \(\mathbf{X}\). We therefore consider estimating parameters \((\vec{\mu}, \mathbb{\Sigma})\) based on an \(n \times p\) data matrix \(\mathbf{X}\) on the assumption that their row vectors jointly follow the MVN distribution</p> \[\vec{x_i} \sim N_p(\vec{\mu}, \mathbb{\Sigma}) (i=1,\dots, n)\] <p>Use ML method for continuous variables to obtain the parameter value that maximizes the probability density of the data being observed. It is because both a probability density and probability stand for how likely it is that a value will be observed. For example, the probability density of \(\vec{x_i} = \vec{x_1}\) is</p> \[\begin{equation} \tag{3} \mathcal{P} (\mathbf{x_1}|\vec{\mu}, \mathbb{\Sigma}) = \frac{1}{(2\pi)^{(p/2)} |\mathbb{\Sigma}|^{1/2}} \exp \left\{- \frac{1}{2} \left(\mathbf{[80,77,68]^T}-\vec{\mu}\right)^{T} \mathbb{\Sigma}^{-1} \left(\mathbf{[80,77,68]^T}-\vec{\mu} \right) \right\} \end{equation}\] <p>On the supposition that the rows of \(\mathbb{X} = [x_1,…,x_n]^T\) are observed mutually independently, the probability density of the n rows in \(\mathbb{X}\) being jointly observed is given by the product of (2)</p> \[\begin{equation} \tag{4} \mathcal{P} (\mathbb{X}|\vec{\mu}, \mathbb{\Sigma}) = \frac{1}{(2\pi)^{(np/2)} |\mathbb{\Sigma}|^{n/2}} \exp \left\{- \frac{1}{2}\sum_{i=1}^{n}(\vec{x_i}-\vec{\mu})^{T} \mathbb{\Sigma}^{-1} \left(\vec{x_i}-\vec{\mu} \right) \right\} \end{equation}\] <h3 id="maximum-likelihood-estimates-of-means-and-covariances">Maximum Likelihood Estimates of Means and Covariances</h3> <p>The \((\vec{\mu}, \mathbb{\Sigma})\) values are obtained in the ML method, such that the data matrix \(\mathbb{X}\) is the most likely to be observed. That is, the maximum likelihood estimates (MLE) of \((\vec{\mu}, \mathbb{\Sigma})\) are estimated that maximizes (4) or its logarithm.</p> \[\tag{5} \begin{equation} log \mathcal{P}(\mathbb{X}|\vec{\mu},\mathbb{\Sigma})=-\frac{np}{2}log2\pi-\frac{n}{2}log|\mathbb{\Sigma}|-\frac{n}{2}\sum_{i=1}^{n}(\vec{x_i}-\vec{\mu})^T\Sigma^T(\vec{x_i}-\vec{\mu}) \end{equation}\] <p>the MLE of \((\vec{\mu}, \mathbb{\Sigma})\) is given by</p> \[\overset{\wedge}{\vec{\mu}}=\bar{\vec{x}}=\frac{1}{n}\sum_{i=1}^{n}\vec{x_i}, \overset{\wedge}{\mathbb{\Sigma}}=\frac{1}{n}\sum_{i=1}^{n}(\vec{x_i}-\bar{\vec{x}})(\vec{x_i}-\bar{\vec{x}})^T=\mathbb{V}\] <p>where MLEs of \((\vec{\mu}, \mathbb{\Sigma})\) are found to be the mean vector and covariance matrix of the data set, respectively.</p> <p>Substituting MLEs into the log likelihood, the maximum likelihood is</p> \[\begin{equation} \tag{6} \mathcal{l}(\overset{\wedge}{\vec{\mu}}, \overset{\wedge}{\mathbb{\Sigma}})=-\frac{n}{2}log(\mid\mathbb{V}\mid)-\frac{np}{2} \end{equation}\] <h3 id="model-selection">Model Selection</h3> <p>Model selection refers to comparing models and selecting the model best fitted to a data set. An advantage of the ML method is its MLE can be used for model selection with statistics called <strong>information criteria</strong>.</p> <ul> <li>Akaike’s information criterion (AIC): \(AIC=-2\mathcal{l}(\overset{\wedge}{\mathbb{\Theta}})+2\eta\)</li> <li>Bayesian information criterion (BIC): \(BIC=-2\mathcal{l}(\overset{\wedge}{\mathbb{\Theta}})+\eta log(n)\)</li> </ul> <p>where \(\mathcal{l}(\overset{\wedge}{\mathbb{\Theta}})\) expresses the value of the log likelihood and \(\eta\) is the number of parameters to be estimated.Both AIC and BIC penalize a model for having more parameters.</p> <ul> <li>Information Criteria and Philosophy: <ul> <li>how well it explains a phenomenon (maximum log likelihood)</li> <li>how simple it is (smallness of number of parameters)</li> </ul> </li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/photo-gallery/">a post with image galleries</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/statistics-notes/">Statistics Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/ML-notes/">Machine Learning Notes</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Tuo Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>